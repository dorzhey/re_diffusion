{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "import blobfile as bf\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = os.path.abspath('')\n",
    "\n",
    "data_dir = os.path.join(file_dir, '..','data_preprocessing','generated_data')\n",
    "\n",
    "train_utils_path = os.path.join(file_dir,'..','train_utils')\n",
    "if train_utils_path not in sys.path:\n",
    "    sys.path.append(train_utils_path)\n",
    "from utils_data import load_TF_data, SequenceDataset\n",
    "\n",
    "universal_guide_path = os.path.join(file_dir, '..', '..','re_design', \n",
    "                                    'Universal-Guided-Diffusion', 'Guided_Diffusion_Imagenet')\n",
    "if universal_guide_path not in sys.path:\n",
    "    sys.path.append(universal_guide_path)\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.resample import create_named_schedule_sampler\n",
    "from guided_diffusion.respace import SpacedDiffusion, space_timesteps\n",
    "from guided_diffusion import gaussian_diffusion as gd\n",
    "\n",
    "classifier_guided_path = os.path.join(file_dir, '..', 'classifier_diffusion')\n",
    "if classifier_guided_path not in sys.path:\n",
    "    sys.path.append(classifier_guided_path)\n",
    "\n",
    "from guided_tools import create_model, create_gaussian_diffusion, create_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonnfruit.models import DragoNNFruit, CellStateController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2024-09-15-21-53-44-836788\n"
     ]
    }
   ],
   "source": [
    "class DiffusionTrainingconfig:\n",
    "    TESTING_MODE = False\n",
    "    data_dir=f'{data_dir}/tcre_seq_motif_cluster.csv'\n",
    "    classifier_checkpoint_path = f'{classifier_guided_path}/classifier_checkpoints/model001000.pt'\n",
    "    schedule_sampler=\"uniform\"\n",
    "    lr=1e-4\n",
    "    weight_decay=0.0\n",
    "    lr_anneal_steps=0\n",
    "    batch_size=512\n",
    "    microbatch=-1  # -1 disables microbatches\n",
    "    ema_rate=\"0.9999\"  # comma-separated list of EMA values\n",
    "    log_interval=100\n",
    "    sample_interval=100\n",
    "    save_interval=5000\n",
    "    resume_checkpoint=\"\"\n",
    "    use_fp16=False\n",
    "    fp16_scale_growth=1e-3\n",
    "\n",
    "    subset = None\n",
    "    num_workers = 8\n",
    "    kmer_length = 5\n",
    "    num_classes = 3\n",
    "    \n",
    "\n",
    "    # model\n",
    "    seq_length=200\n",
    "    image_size=seq_length\n",
    "    \n",
    "    num_channels=128\n",
    "    num_res_blocks=2\n",
    "    num_heads=4\n",
    "    num_heads_upsample=-1\n",
    "    num_head_channels=-1\n",
    "    attention_resolutions=\"100,50,25\"\n",
    "    channel_mult=\"\"\n",
    "    dropout=0.0\n",
    "    class_cond=True\n",
    "    use_checkpoint=False\n",
    "    use_scale_shift_norm=True\n",
    "    resblock_updown=True\n",
    "    use_fp16=False\n",
    "    use_new_attention_order=False\n",
    "\n",
    "    # Diffusion\n",
    "    learn_sigma=False\n",
    "    diffusion_steps=100\n",
    "    noise_schedule=\"linear\"\n",
    "    timestep_respacing=\"\"\n",
    "    sigma_small=False\n",
    "    use_kl=True\n",
    "    predict_xstart=False\n",
    "    rescale_timesteps=False\n",
    "    rescale_learned_sigmas=False\n",
    "\n",
    "    # Sampling\n",
    "    use_ddim=False\n",
    "    clip_denoised=True\n",
    "    num_sampling_to_compare_cells = 10\n",
    "    sample_bs = 1\n",
    "    use_classifier = True\n",
    "    classifier_scale=10 # between 1 and 10 trade off between diversity and fidelity\n",
    "    run_name = \"\" # for gimme scan erros when doing several runs simultaneously \n",
    "\n",
    "    # Classifier\n",
    "    classifier_use_fp16=False\n",
    "    classifier_width=256\n",
    "    classifier_depth=3\n",
    "    classifier_attention_resolutions=\"100,50,25\"  # 16\n",
    "    classifier_use_scale_shift_norm=True  # False\n",
    "    classifier_resblock_updown=True  # False\n",
    "    classifier_pool=\"spatial\"\n",
    "\n",
    "\n",
    "config = DiffusionTrainingconfig()\n",
    "\n",
    "dist_util.setup_dist()\n",
    "logger.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating data loader...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating data loader...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_TF_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_total_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_sampling_to_compare_cells\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_sampling_to_compare_cells\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_save_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcre_encode_data_motif_cluster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msaved_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcre_encode_data_motif_cluster.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_saved_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_label_number\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/projects/re_diffusion/universal_guidance/../train_utils/utils_data.py:66\u001b[0m, in \u001b[0;36mload_TF_data\u001b[0;34m(data_path, seqlen, saved_file_name, limit_total_sequences, num_sampling_to_compare_cells, to_save_file_name, load_saved_data, start_label_number)\u001b[0m\n\u001b[1;32m     63\u001b[0m train_sequences_cell_specific \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     65\u001b[0m x_train_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([one_hot_encode_dna_sequence(x, seqlen) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m---> 66\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_train_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m X_train[X_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Creating labels\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.log(\"creating data loader...\")\n",
    "data = load_TF_data(\n",
    "    data_path=config.data_dir,\n",
    "    seqlen=config.seq_length,\n",
    "    limit_total_sequences=config.subset,\n",
    "    num_sampling_to_compare_cells=config.num_sampling_to_compare_cells,\n",
    "    to_save_file_name=\"cre_encode_data_motif_cluster\",\n",
    "    saved_file_name=\"cre_encode_data_motif_cluster.pkl\",\n",
    "    load_saved_data=True,\n",
    "    start_label_number = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from universal_tools import (\n",
    "    create_model_and_diffusion,\n",
    "    OperationArgs, \n",
    "    SamplingArgs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model and diffusion...\n",
      "creating model and diffusion...\n"
     ]
    }
   ],
   "source": [
    "logger.log(\"creating model and diffusion...\")\n",
    "model, diffusion = create_model_and_diffusion(config)\n",
    "\n",
    "model.to(dist_util.dev())\n",
    "\n",
    "schedule_sampler = create_named_schedule_sampler(config.schedule_sampler, diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedDynamicBPNetCounts(torch.nn.Module):\n",
    "\n",
    "\tdef __init__(self, controller, n_filters=128, n_layers=8, trimming=None, \n",
    "\t\tconv_bias=False, n_outputs=1, dropout_rate=0.2):\n",
    "\t\tsuper(RegularizedDynamicBPNetCounts, self).__init__()\n",
    "\n",
    "\t\tself.trimming = trimming if trimming is not None else 2 ** n_layers + 37\n",
    "\t\tself.n_filters = n_filters\n",
    "\t\tself.n_layers = n_layers\n",
    "\t\tself.n_outputs = n_outputs\n",
    "\t\tself.dropout_rate = dropout_rate\n",
    "\t\tself.iconv = torch.nn.Conv1d(4, n_filters, kernel_size=21, padding=10)\n",
    "\t\tself.irelu = torch.nn.ReLU()\n",
    "\t\tself.idropout = torch.nn.Dropout(p=dropout_rate)\n",
    "  \t\t\n",
    "\t\tself.deconv_kernel_size = 75\n",
    "\t\tself.fconv = torch.nn.Conv1d(self.n_filters, self.n_outputs, kernel_size=self.deconv_kernel_size,\n",
    "\t\t\tbias=conv_bias)\n",
    "\n",
    "\t\tself.biases = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.Linear(controller.n_outputs, n_filters) for i in range(\n",
    "\t\t\t\tn_layers)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.convs = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.Conv1d(n_filters, n_filters, kernel_size=3, stride=1, \n",
    "\t\t\t\tdilation=2**i, padding=2**i, bias=conv_bias) for i in range(1, \n",
    "\t\t\t\t\tn_layers+1)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.relus = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.ReLU() for i in range(n_layers)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.linear = torch.nn.Linear(n_filters, 1)\n",
    "\n",
    "\t\tself.dropouts = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.Dropout(p=dropout_rate) for _ in range(n_layers)\n",
    "\t\t])\n",
    "\t   \n",
    "\t\tself.controller = controller\n",
    "\n",
    "\tdef forward(self, X, cell_states):\n",
    "\n",
    "\t\tstart, end = self.trimming, X.shape[2] - self.trimming\n",
    "\t\tcell_states = self.controller(cell_states)\n",
    "\t\tX = self.irelu(self.iconv(X))\n",
    "\t\tX = self.idropout(X)\n",
    "\t\tfor i in range(self.n_layers):\n",
    "\t\t\tX_conv = self.convs[i](X)\n",
    "\t\t\tX_bias = self.biases[i](cell_states).unsqueeze(-1)\t\t\t\n",
    "\t\t\tX = X + self.relus[i](X_conv + X_bias)\n",
    "\t\t\tX = self.dropouts[i](X)\n",
    "\n",
    "\t\tX = X[:, :, start - self.deconv_kernel_size//2 : end + self.deconv_kernel_size//2]\n",
    "\t\ty_profile = self.fconv(X)\n",
    "\t\tX = torch.mean(X, axis=2)\n",
    "\t\ty_counts = self.linear(X)\n",
    "\n",
    "\t\treturn y_profile, y_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scBPnetCounts(DragoNNFruit):\n",
    "\tdef __init__(self, accessibility, name, alpha=1, scale_log_rd=False, n_outputs=2):\n",
    "\t\ttorch.nn.Module.__init__(self)\n",
    "\t\tself.accessibility = accessibility\n",
    "\t\tself.name = name\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.n_outputs = n_outputs\n",
    "\t\tself.scale_log_rd = scale_log_rd\n",
    "\t\t# self.logger = Logger([\"Epoch\", \"Iteration\", \"Training Time\",\n",
    "\t\t# \t\"Validation Time\", \"Training MNLL\", \"Validation MNLL\",\n",
    "\t\t# \t\"Validation Profile Correlation\", \"Validation Count Correlation\", \n",
    "\t\t# \t\"Saved?\"], verbose=True)\n",
    "\n",
    "\tdef forward(self, X, cell_states):\n",
    "\n",
    "\t\treturn self.accessibility(X, cell_states)\n",
    "\n",
    "\tdef log_softmax(self, y_profile):\n",
    "\t\ty_profile = y_profile.reshape(y_profile.shape[0], -1)\n",
    "\t\ty_profile = torch.nn.LogSoftmax(dim=-1)(y_profile)\n",
    "\t\ty_profile = y_profile.reshape(y_profile.shape[0], self.n_outputs, -1)\n",
    "\t\treturn y_profile\n",
    "\t\n",
    "\tdef predict(self, X, cell_states, batch_size=64, logits = False):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tstarts = np.arange(0, X.shape[0], batch_size)\n",
    "\n",
    "\t\t\tends = starts + batch_size\n",
    "\n",
    "\t\t\ty_profiles, y_counts = [], []\n",
    "\t\t\tfor start, end in zip(starts, ends):\n",
    "\t\t\t\tX_batch = X[start:end]\n",
    "\t\t\t\tcell_states_batch = cell_states[start:end]\n",
    "\t\t\t\ty_profiles_, y_counts_ = self(X_batch,cell_states_batch)\n",
    "\t\t\t\tif not logits:  # apply softmax\n",
    "\t\t\t\t\ty_profiles_ = self.log_softmax(y_profiles_)\n",
    "\t\t\t\ty_profiles.append(y_profiles_.cpu().detach().numpy())\n",
    "\t\t\t\ty_counts.append(y_counts_.cpu().detach().numpy())\n",
    "\n",
    "\t\t\ty_profiles = np.concatenate(y_profiles)\n",
    "\t\t\ty_counts = np.concatenate(y_counts)\n",
    "\t\t\treturn y_profiles, y_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_config = {\n",
    "    'cell_state_dim':50,\n",
    "    'ctrl_nodes':256,\n",
    "    'ctrl_layers':1, #0\n",
    "    'ctrl_outputs':64,\n",
    "    'bp_n_filters':128,\n",
    "    'conv_layers':8,\n",
    "    'trimming':None,\n",
    "    'dropout_rate':0.2,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = CellStateController(\n",
    "\t\tn_inputs=guide_config['cell_state_dim'], \n",
    "\t\tn_nodes=guide_config['ctrl_nodes'], \n",
    "\t\tn_layers=guide_config['ctrl_layers'], \n",
    "\t\tn_outputs=guide_config['ctrl_outputs'],\n",
    " \t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessibility_model = RegularizedDynamicBPNetCounts(\n",
    "\t\t\tcontroller=controller,\n",
    "\t\t\tn_filters=guide_config['bp_n_filters'], \n",
    "\t\t\tn_layers=guide_config['conv_layers'], \n",
    "\t\t\ttrimming=guide_config['trimming'], \n",
    "\t\t\tdropout_rate=guide_config['dropout_rate'], \n",
    "\t\t\tn_outputs=2,\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_function = scBPnetCounts(accessibility_model, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(x, t, y=None, args=None, model=None):\n",
    "    # assert y is not None\n",
    "    return model(x, t, y if args.class_cond else None)\n",
    "\n",
    "def cond_fn(x, t, y=None, args=None, guide_function=None):\n",
    "    assert y is not None\n",
    "    with torch.enable_grad():\n",
    "        x_in = x.detach().requires_grad_(True)\n",
    "        logits = guide_function(x_in, t)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        selected = log_probs[range(len(logits)), y.view(-1)]\n",
    "        return torch.autograd.grad(selected.sum(), x_in)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_config = OperationArgs()\n",
    "sampling_config = SamplingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 samples for cell_type ct2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.73it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 45.00it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 46.51it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 47.82it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 49.01it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 47.71it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 46.99it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 50.29it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 46.41it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 43.75it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_motifs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m         final_sequences\u001b[38;5;241m.\u001b[39mappend(seq_final)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# extract motifs from generated sequences\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m df_motifs_count_syn \u001b[38;5;241m=\u001b[39m \u001b[43mextract_motifs\u001b[49m(final_sequences)\n\u001b[1;32m     49\u001b[0m generated_celltype_motif[cell_type] \u001b[38;5;241m=\u001b[39m df_motifs_count_syn\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_motifs' is not defined"
     ]
    }
   ],
   "source": [
    "cell_num_list = data['cell_types']\n",
    "cell_list = list(data[\"numeric_to_tag\"].values())\n",
    "nucleotides = [\"A\", \"C\", \"G\", \"T\"]\n",
    "sample_bs = config.sample_bs\n",
    "num_samples = round(config.num_sampling_to_compare_cells/sample_bs)\n",
    "generated_celltype_motif = {}\n",
    "seqlen = config.seq_length\n",
    "model.eval()\n",
    "# iterate over cell types\n",
    "for cell_num in cell_num_list:\n",
    "    cell_type = data['numeric_to_tag'][cell_num]\n",
    "    print(f\"Generating {config.num_sampling_to_compare_cells} samples for cell_type {cell_type}\")\n",
    "    final_sequences = []\n",
    "    for n_a in range(num_samples):\n",
    "        model_kwargs = {}\n",
    "        sampled_cell_types = np.array([cell_num] * sample_bs)\n",
    "        classes = torch.from_numpy(sampled_cell_types).to(dist_util.dev())\n",
    "        model_kwargs[\"y\"] = classes\n",
    "        sample = diffusion.ddim_sample_loop_operation(\n",
    "            partial(model_fn, model=model, args=config),\n",
    "            (config.sample_bs, 1, 4, config.image_size),\n",
    "            operated_image=None,\n",
    "            operation=operation_config,\n",
    "            clip_denoised=config.clip_denoised,\n",
    "            model_kwargs=model_kwargs,\n",
    "            cond_fn=partial(cond_fn, guide_function=guide_function),\n",
    "            device=torch.device('cuda'),\n",
    "            progress=sampling_config.progressive\n",
    "        ).squeeze(1)\n",
    "        for n_b, x in enumerate(sample):\n",
    "            sequence = \"\".join([nucleotides[s] for s in np.argmax(x.detach().cpu(), axis=0)])\n",
    "            seq_final = f\">seq_test_{n_a}_{n_b}\\n\" + sequence\n",
    "            final_sequences.append(seq_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>seq_test_0_0\\nAAGAAAAAAAAACAGACCAAATAAAAACGGCCAAGAAATAAACACTACCAATCGAGGCAGCTAAGATACACCAGCCATATAACAACAAAGCGACATCAAAAGACCAAAAACACAAAACAACCAAGGAAATGAAAAACGACAAACCACCAGGAATCCACCAAATCCACCACATCAAGCCACCATCCGAACACAACTACGCA',\n",
       " '>seq_test_1_0\\nACAACATACCTGCCACAAAAACAAACCAGCACAACAGAATGACAAACAAAAAACCTAAACACACACGAAGCAATGCCCAAAAACTAAGCAGCAACCCCAAAGGAGAAAAAAAGCAAACGCACAACAAACCGAATCCAGAAAACCATCACTGCAGCAGCGACAAAAGGGAACCTGACAATCCCCAAGAACGAAAGACAGGA',\n",
       " '>seq_test_2_0\\nAATGAGAATGCCAACAGAAAGACGAACCACACAATAACGCGCAGCATGAATGAACAGACAACAGAAAAAACACAAACAGAACAGCAAACCTCAGAAAAACAGAACAAAAAAACAACACAACAAAAAGACTAAAAAAGACAAAGAGAACTGGAAAAACCAAAAAAAACGATCCCCCAGACAAAAGTCAACAAAACAAAAAA',\n",
       " '>seq_test_3_0\\nAAATCAAGTAGCAGGACGAAGATAAAGCCAATAAAAACAAAAGCCAAGAAAAAAAAATGCCCTAAACTCGCGCACAAGGAGACAACAAATTCCGAAAAGAAACAAAGAGACAAAATCAAAGAAATCGTGACACACAACAAAGTACAGAAATACAAAACAAATCCAAACAAAACCAGAAAGAATGGAAAACTACGACTGAT',\n",
       " '>seq_test_4_0\\nCCGGCCCACAGAAAGCACGAACACCCATATCACTAACGAAACAGGCATACACCACAAGCTCACACTTGAGGCAAAGAAACCAAAAACAACCAGAACCACAACAATAACACTTAACAACAACAAAAAAAGAGCAATAAAACTGCAGAGCGAACTAGACAGAACCAAAGACAGTACTACGAAAGAGACCAATGGAAAAAAAA',\n",
       " '>seq_test_5_0\\nACGCTACGCAACGCAACCACAAGTAAAAACACGATCCAGAAAACAAACAGCACCCAACCAAAGAACAAAACAAGAAAGAACCACAAATACGCACGCAACTACAAAGAGCAGACACAGACCAGGAGACATAAAATGAAAACAACGGAACAGAAAGAGACACCAAAACCTACGCAATGATAGAAACAAATGGACCAAATAGG',\n",
       " '>seq_test_6_0\\nAGGACCGAGGAAAAAAAAAGAACCGTAAAAACAAAAGACAGGAACGAAAGAGCCAACTGCATGACACCCAGCAAAACGGCAACGCAAACCCAGTACAAAAAAACAAGGAAAAAAAGAGGAAAATACCACAAGAAGAACCAACGAACCAACACCTAACAGAGTTAGAAATACCACCAGAAAAAAAACGAATACACGAAAGC',\n",
       " '>seq_test_7_0\\nAGACACCAACAAAACAATAAAAAGAGAGCAACCCACACCACCACTACATACAAAACATGAACAAAAGAAGCAAAGCAACCCTCACATAGAAAAAAACCAACCAACAAATAAGGACCCAAAAAATAAAAACAACAACATAAAAAAATCTAGACGAGGAAAAGGACAGTACCGTTATTAAAAAGACAGATCACAAAACGAGC',\n",
       " '>seq_test_8_0\\nATCAATCACCGCATCAAATCAGACAAAAAACCCAACAAGGCCAAAAAAAAGAGCAAACAGCAGACAGGAAAAACCAGGCGAGAACCCCAGAAACCAACCTGAACCGCACAACAGACAAAAAAGGCAAAAAGAAAACCAAACCCCCGAAAAAACAAACCAGCAAAACACAGTAAAAGAAACCCAATTAGAAAAAACGAAAA',\n",
       " '>seq_test_9_0\\nCCTAGACAGAAGAGACAACAGAAAAACCAAAAAGCGAACAAGCTCAAAGCAAGTGCTATACCAAAACACCCCCACAAAAACAGAAAGGGATGCAGAACACCAAACAAAACTTACTCGAGCAAAAACCAAATACAAACCGAACAAGAAAGGGTGAAGATACAACAACAAAACACGAAAAAAAATACAAGAAAACGAGAAAA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
